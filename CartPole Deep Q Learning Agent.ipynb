{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59284e03",
   "metadata": {},
   "source": [
    "## Feel free to change the self.render and self.load_model to change the window displaying and the optimal weights i converged from being initially loaded, thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c478e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided Imports.\n",
    "import gym\n",
    "from gym.envs.classic_control.cartpole import *\n",
    "from pyglet.window import key\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# MY IMPORTS - ALL SHOULD BE EASILY AVAILABLE WITHIN ANACONDA.\n",
    "import statistics\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Variable initialization.\n",
    "number_of_trials = 100 # This is 100 so as to ensure that the converged model will be shown for ep 50-100 worst case.\n",
    "\n",
    "bool_do_not_quit = True  # Boolean to quit pyglet.\n",
    "\n",
    "# Lists.\n",
    "scores = []  # Your gaming score\n",
    "episodes = [] # List containing all episodes played.\n",
    "\n",
    "\n",
    "# Function that maps keys to program behaviour.\n",
    "def key_press(k, mod):\n",
    "    global bool_do_not_quit, a, restart\n",
    "    if k == 0xff0d: restart = True    # Corresponds to the Enter Key.\n",
    "    if k == key.ESCAPE: bool_do_not_quit = False\n",
    "    if k == key.Q: bool_do_not_quit = False\n",
    "        \n",
    "\n",
    "# The class that defines the Deep Q Network Agent.\n",
    "class DeepQNetworkAgent:\n",
    "    # Constructor (self) initialising all the required parameters.\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Change these to true to see the game render / load the optimal weights that converge quickly. \n",
    "        self.render = False\n",
    "        self.load_model = True\n",
    "\n",
    "        # Establish the number of inputs and actions (4 inputs (observation values) | 2 actions - Left/Right).\n",
    "        self.state_size = state_size    # 4.\n",
    "        self.action_size = action_size  # 2.\n",
    "\n",
    "        # These are hyper parameters for the DQN.\n",
    "        self.discount_factor = 0.99 # Reward relative to position in time (future).\n",
    "        self.learning_rate = 0.001  # Step size per iteration.\n",
    "        self.epsilon = 1.0          # Epsilon greedy parameters below.\n",
    "        self.epsilon_decay = 0.999  # Value that epsilon is multiplied by to hone behaviour.\n",
    "        self.epsilon_min = 0.01     # The minimum amount epsilon can be.\n",
    "        self.batch_size = 64        # The number of samples processed before the model is updated.\n",
    "        self.train_start = 1000     # Memory value allocated for training.\n",
    "        \n",
    "        self.memory = deque(maxlen=2000) # The models memory, using the deque container.\n",
    "\n",
    "        # Create main model and target model.\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # Initialize target model.\n",
    "        self.update_target_model()\n",
    "        \n",
    "        # loads the optimal paramaters, if they are saved.\n",
    "        if self.load_model:\n",
    "            self.model = keras.models.load_model(\"./best_dnq.h5\")\n",
    "\n",
    "\n",
    "    # Standard sequential neural network.\n",
    "    def build_model(self):\n",
    "        model = Sequential() # Model type\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform')) # Input layer\n",
    "        model.add(Dense(24, activation='relu', kernel_initializer='he_uniform'))                            # Hidden layer\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))            # Output layer\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate)) # Compile the model\n",
    "        return model\n",
    "\n",
    "\n",
    "    # Update the target models weights to be the same as the models current weights.\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    # Decide which action to choose, by using the epsilon-greedy policy.\n",
    "    def decide(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "\n",
    "    # Save the current samples parameters to the models memory in deque.\n",
    "    def add_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    # Randomly select a sample from the models memory, using a mini batch sample.\n",
    "    def train_model(self):\n",
    "        # If still in the allocated training memory portion : return.\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        # Take a sample based upon the batch size (64).\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Create two numpy arrays that are empty (0's).\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        \n",
    "        # list creation of our parameters.\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        # loop through for the batch size, updating and appending our parameters.\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "        \n",
    "        # Assign the new target values based upon the input from the mini batch sample.\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "        \n",
    "        # Get the maximum Q value from the target model.\n",
    "        for i in range(self.batch_size):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "\n",
    "        # Finally, we can fit the model with our updated input as well as the target, for one iteration (epoch)\n",
    "        self.model.fit(update_input, target, batch_size = self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "def run_cartPole_asAgent():\n",
    "    # Maximum score = 200 for v0.\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    # Get the number of states and actions from environment (4S : 2A)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Initialise our agent with the states and actions.\n",
    "    agent = DeepQNetworkAgent(state_size, action_size)\n",
    "    \n",
    "    # Main loop - houses the repetition for each episode (trial)\n",
    "    for e in range(number_of_trials):\n",
    "        done = False        # Boolean, is the episode over?\n",
    "        score = 0           # Measurement of agent performance.\n",
    "        steps = -1   # Steps is always 1 above so its better to display 199 instead of 200, rather than always +1 for !200.\n",
    "        t1 = time.time()    # Keeps track of the time the agent is active within our CartPole Env.\n",
    "        \n",
    "        # Reset our enviorment, and therefore associated action.\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        # Secondary loop - houses the behaviour while the agent is active.\n",
    "        while not done:\n",
    "            # Early escape should quit keys be pressed.\n",
    "            if bool_do_not_quit == False:\n",
    "                sys.exit()\n",
    "            \n",
    "            # If the CartPole enviroment is being displayed, render it, and capture user input (key_press function).\n",
    "            if agent.render:\n",
    "                env.render();\n",
    "                env.viewer.window.on_key_press = key_press\n",
    "\n",
    "            # Get the current action, and assign the new parameters with the next step.\n",
    "            action = agent.decide(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            # If the agent's action causes the program to end (mistake), then penalise the agent.\n",
    "            reward = reward if not done or score == 199 else -40\n",
    "\n",
    "            # Save the current episode to the model memory in deque.\n",
    "            agent.add_sample(state, action, reward, next_state, done)\n",
    "\n",
    "            # Train the model based upon the prior actions.\n",
    "            agent.train_model()\n",
    "            \n",
    "            # Update the score / state / step.\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            # When the agent is done:\n",
    "            if done:\n",
    "                # Record the time taken.\n",
    "                t1 = time.time()-t1\n",
    "                \n",
    "                # Update the target model for the next episode.\n",
    "                agent.update_target_model()\n",
    "                \n",
    "                # Append the score to the list for averaging and displaying below.\n",
    "                score = score if score == 200 else score + 40\n",
    "                scores.append(score)\n",
    "                \n",
    "                # Append the episode to the list container, so that the total amount is always know, even with early exit.\n",
    "                episodes.append(e)\n",
    "                \n",
    "                # Display the episodes score to the console.\n",
    "                print(\"Episode\", e, \"| Score:\", score, '|', steps, \"steps | %0.2fs.\"% t1)\n",
    "\n",
    "                # save weights externally if performance is nearly optimal.\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 195:\n",
    "                    agent.model.save(\"./best_dnq.h5\")\n",
    "                    \n",
    "    # Close the gym env when done.                \n",
    "    env.close()        \n",
    "\n",
    "\n",
    "# Call the agent and run CartPole V0.\n",
    "run_cartPole_asAgent()            \n",
    "\n",
    "# Display the average score for the amount of episodes completed.\n",
    "print(\"\\n\\nAverage Score for \", len(episodes), \" episodes is:\",statistics.mean(scores))\n",
    "\n",
    "# Plot the models score over time.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.title('My agent performance on CartPole-v0')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Agent Episode')\n",
    "plt.show();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
